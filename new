# Install required libraries
!pip install PyPDF2 transformers faiss-cpu sentence-transformers

# Import libraries
import PyPDF2
import torch
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Step 1: Extract text from PDF
def extract_text_from_pdf(pdf_path):
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    return text

# Step 2: Create embeddings and FAISS index
def create_faiss_index(text_chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(text_chunks)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index, model

# Step 3: Retrieve relevant text chunks
def retrieve_relevant_chunks(query, index, model, text_chunks, k=3):
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k)
    relevant_chunks = [text_chunks[idx] for idx in indices[0]]
    return relevant_chunks

# Step 4: Generate answer using RAG
def generate_answer(query, relevant_chunks):
    tokenizer = RagTokenizer.from_pretrained("facebook/rag-sequence-nq")
    retriever = RagRetriever.from_pretrained("facebook/rag-sequence-nq", index_name="custom", passages=relevant_chunks)
    model = RagSequenceForGeneration.from_pretrained("facebook/rag-sequence-nq", retriever=retriever)
    
    inputs = tokenizer(query, return_tensors="pt")
    outputs = model.generate(input_ids=inputs["input_ids"])
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# Step 5: Simple RAG pipeline
def simple_rag_pipeline(pdf_path, query):
    # Step 1: Extract text from PDF
    text = extract_text_from_pdf(pdf_path)
    
    # Step 2: Create embeddings and FAISS index
    text_chunks = text.split('. ')  # Simple text splitting by sentences
    index, model = create_faiss_index(text_chunks)
    
    # Step 3: Retrieve relevant chunks
    relevant_chunks = retrieve_relevant_chunks(query, index, model, text_chunks)
    
    # Step 4: Generate answer using RAG
    answer = generate_answer(query, relevant_chunks)
    
    return answer

# Example usage
pdf_path = "example.pdf"  # Upload your PDF to Colab and replace with the correct path
query = "What is the main topic of the document?"

# Run the pipeline
answer = simple_rag_pipeline(pdf_path, query)
print("Answer:", answer)
